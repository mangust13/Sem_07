# -*- coding: utf-8 -*-
"""AI_04

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EgmuPAf47ebXVy7jKUPqHZNviOzc48TG
"""

# --- Setup, Imports, Mount ---
!pip install datasets transformers jiwer soundfile librosa accelerate evaluate torchcodec matplotlib

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import torch
import librosa
import evaluate
import matplotlib.pyplot as plt
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import re
from sklearn.model_selection import train_test_split

# --- Load TSV + Split ---
dataset_path = "/content/drive/MyDrive/Colab Notebooks/Colab_Data/Lab_04Audio/Lab_04Dataset.tsv"
clips_dir = "/content/drive/MyDrive/Colab Notebooks/Colab_Data/Lab_04Audio/clips"

df = pd.read_csv(dataset_path, sep="\t")

train_df, test_df = train_test_split(df, test_size=0.1, shuffle=True, random_state=42)

# --- Text Cleaning + Preprocess Audio ---
def clean_label(text):
    text = str(text).lower()
    text = text.replace("—", " ")
    text = text.replace("–", " ")
    text = text.replace("...", " ")
    text = text.replace("…", " ")
    text = text.replace(".", " ")
    text = re.sub(r'[^\w\sʼ-]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


train_inputs, train_labels_raw = [], []
test_inputs, test_labels_raw = [], []

for i in tqdm(range(len(train_df))):
    row = train_df.iloc[i]
    p = os.path.join(clips_dir, row["path"])
    if not os.path.exists(p):
        continue
    a, _ = librosa.load(p, sr=16000, mono=True)
    train_inputs.append(a)
    train_labels_raw.append(str(row["sentence"]))

for i in tqdm(range(len(test_df))):
    row = test_df.iloc[i]
    p = os.path.join(clips_dir, row["path"])
    if not os.path.exists(p):
        continue
    a, _ = librosa.load(p, sr=16000, mono=True)
    test_inputs.append(a)
    test_labels_raw.append(str(row["sentence"]))

train_labels = [clean_label(t) for t in train_labels_raw]
test_labels = [clean_label(t) for t in test_labels_raw]

# --- Metrics + Inference Helper ---
wer = evaluate.load("wer")
cer = evaluate.load("cer")

def infer_one(audio, processor, model, device):
    inp = processor(audio, sampling_rate=16000, return_tensors="pt")
    inp = {k: v.to(device) for k, v in inp.items()}
    with torch.no_grad():
        out = model.generate(
            inp["input_features"],
            max_length=256,
            num_beams=1,
        )
    return clean_label(processor.batch_decode(out, skip_special_tokens=True)[0])

# --- Evaluate 3 Models BEFORE FT ---
models_to_test = {
    "tiny": "openai/whisper-tiny",
    "base": "openai/whisper-base",
    "small": "openai/whisper-small"
}

before_results = {}

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Використовується пристрій: {device}")

num_test_samples = 30
if len(test_inputs) < num_test_samples:
    num_test_samples = len(test_inputs)

for name, mname in models_to_test.items():
    print(f"\nТестування моделі: {name}")
    processor = WhisperProcessor.from_pretrained(mname, language="uk", task="transcribe")
    model = WhisperForConditionalGeneration.from_pretrained(mname).to(device)
    preds, refs = [], []

    for i in tqdm(range(num_test_samples)):
        preds.append(infer_one(test_inputs[i], processor, model, device))
        refs.append(test_labels[i])

    before_results[name] = {
        "wer": wer.compute(predictions=preds, references=refs),
        "cer": cer.compute(predictions=preds, references=refs)
    }
    print(f"Результат для {name}: WER={before_results[name]['wer']:.4f}, CER={before_results[name]['cer']:.4f}")

# --- Fine-Tuning Whisper Tiny ---
MODEL_NAME_FOR_FT = "openai/whisper-tiny"

processor = WhisperProcessor.from_pretrained(MODEL_NAME_FOR_FT, language="uk", task="transcribe")
model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME_FOR_FT).to(device)

train_label_ids = [processor.tokenizer(t).input_ids[:160] for t in train_labels]
test_label_ids = [processor.tokenizer(t).input_ids[:128] for t in test_labels]

class WhisperTrainDataset(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y
    def __len__(self):
        return len(self.x)
    def __getitem__(self, idx):
        return {"audio": self.x[idx], "labels": self.y[idx]}

def collate_fn(batch):
    audios = [b["audio"] for b in batch]
    labels = [b["labels"] for b in batch]

    feat = processor.feature_extractor(audios, sampling_rate=16000, return_tensors="pt")
    lab = processor.tokenizer.pad({"input_ids": labels}, padding=True, return_tensors="pt")["input_ids"]
    lab[lab == processor.tokenizer.pad_token_id] = -100

    return {
        "input_features": feat["input_features"],
        "labels": lab
    }

train_dataset = WhisperTrainDataset(train_inputs, train_label_ids)
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-6)

model.train()
num_epochs = 3

for epoch in range(num_epochs):
    total_loss = 0
    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        inp = batch["input_features"].to(device)
        lab = batch["labels"].to(device)

        optimizer.zero_grad()
        out = model(input_features=inp, labels=lab)
        loss = out.loss
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1} loss: {total_loss / len(train_loader):.4f}")

# --- Evaluate AFTER FT + Plots ---
model.eval()

preds_after = []
refs_after = []

for i in tqdm(range(num_test_samples)):
    preds_after.append(infer_one(test_inputs[i], processor, model, device))
    refs_after.append(test_labels[i])

after_results = {
    "wer": wer.compute(predictions=preds_after, references=refs_after),
    "cer": cer.compute(predictions=preds_after, references=refs_after),
}
print(f"Результат після FT: WER={after_results['wer']:.4f}, CER={after_results['cer']:.4f}")

models = ["tiny", "base", "small", "tiny_finetuned"]
wers = [
    before_results["tiny"]["wer"],
    before_results["base"]["wer"],
    before_results["small"]["wer"],
    after_results["wer"]
]

plt.figure(figsize=(10,5))
plt.bar(models, wers, color=["gray","blue","green","red"])
plt.title("WER Before/After Training (Whisper Tiny Fine-Tuned)")
plt.ylabel("WER")
plt.show()

cers = [
    before_results["tiny"]["cer"],
    before_results["base"]["cer"],
    before_results["small"]["cer"],
    after_results["cer"]
]

plt.figure(figsize=(10,5))
plt.bar(models, cers, color=["gray","blue","green","red"])
plt.title("CER Before/After Training (Whisper Tiny Fine-Tuned)")
plt.ylabel("CER")
plt.show()
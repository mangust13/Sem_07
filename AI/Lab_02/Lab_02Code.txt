# --- Library Imports ---
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from google.colab import drive
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, clone_model
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2

# --- Data Preparation ---
drive.mount('/content/drive')

data_path = "/content/drive/MyDrive/Colab Notebooks/Colab_Data/Lab_02Data.tsv"

df = pd.read_csv(data_path, sep='\t')
df = df.dropna(subset=['text'])

emotion_cols = ["Joy","Fear","Anger","Sadness","Surprise"]

X = df["text"].astype(str).values
y = df[emotion_cols].values

tokenizer = Tokenizer(num_words=20000, oov_token="<OOV>")
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)

max_len = 50
X_pad = pad_sequences(X_seq, maxlen=max_len)


# --- Data Split and Class Weights ---
X_train, X_val, y_train, y_val = train_test_split(X_pad, y, test_size=0.2, random_state=42)

neg = np.sum(y_train == 0)
pos = np.sum(y_train == 1)
total = neg + pos

print(f"\n--- Class Balance (5 classes, cleaned data) ---")
print(f"Training samples: {len(X_train)} | Validation samples: {len(X_val)}")

weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)
class_weights = {0: weight_for_0, 1: weight_for_1}


# --- Demo Sentences ---
sample_texts = [
    "Я сьогодні дуже щасливий!",
    "Мені страшно за майбутнє.",
    "Я ненавиджу тебе, ти мене бісиш.", 
    "Сумно, коли друзі далеко.",
    "Вау, я такого не очікував!"
]
sample_seq = tokenizer.texts_to_sequences(sample_texts)
sample_pad = pad_sequences(sample_seq, maxlen=max_len)


# --- Build Base LSTM Model ---
base_model_lstm = Sequential([
    Embedding(input_dim=20000, output_dim=128),
    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(5, activation='sigmoid')
])

opt_lstm = Adam(learning_rate=0.0005)
base_model_lstm.compile(loss='binary_crossentropy', optimizer=opt_lstm, metrics=['accuracy'])


# --- Untrained LSTM Evaluation ---
print("\n=== Untrained LSTM Predictions (Sample Sentences) ===")
untrained_preds_lstm = base_model_lstm.predict(sample_pad)
for text, pred in zip(sample_texts, untrained_preds_lstm):
    emotions = dict(zip(emotion_cols, [round(float(p), 3) for p in pred]))
    print(f"\nText: {text}\nPredicted emotions: {emotions}")

print("\n=== Untrained LSTM Evaluation ===")
loss_un, acc_un = base_model_lstm.evaluate(X_val, y_val)
print(f"Untrained Loss: {loss_un:.4f}, Untrained Accuracy: {acc_un:.4f}")


# --- Train LSTM Model ---
print("\n--- Training LSTM ---")

model_lstm = clone_model(base_model_lstm)
model_lstm.set_weights(base_model_lstm.get_weights())

opt_clone_lstm = Adam(learning_rate=0.0005)
model_lstm.compile(loss='binary_crossentropy', optimizer=opt_clone_lstm, metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history_lstm = model_lstm.fit(
    X_train, y_train,
    epochs=25,
    batch_size=64,
    validation_data=(X_val, y_val),
    callbacks=[early_stop],
    class_weight=class_weights
)


# --- Trained LSTM Evaluation ---
print("\n=== Trained LSTM Predictions (Sample Sentences) ===")
trained_preds_lstm = model_lstm.predict(sample_pad)
for text, pred in zip(sample_texts, trained_preds_lstm):
    emotions = dict(zip(emotion_cols, [round(float(p), 3) for p in pred]))
    print(f"\nText: {text}\nPredicted emotions: {emotions}")

print("\n=== Trained LSTM Evaluation ===")
loss_tr, acc_tr = model_lstm.evaluate(X_val, y_val)
print(f"Trained Loss: {loss_tr:.4f}, Trained Accuracy: {acc_tr:.4f}")
print("---------------------------------")


# --- Build Base GRU Model ---
base_model_gru = Sequential([
    Embedding(input_dim=20000, output_dim=128),
    Bidirectional(GRU(64, dropout=0.3, recurrent_dropout=0.3)),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.5),
    Dense(5, activation='sigmoid')
])

opt_gru = Adam(learning_rate=0.0005)
base_model_gru.compile(loss='binary_crossentropy', optimizer=opt_gru, metrics=['accuracy'])


# --- Untrained GRU Evaluation ---
print("\n=== Untrained GRU Predictions (Sample Sentences) ===")
untrained_preds_gru = base_model_gru.predict(sample_pad)
for text, pred in zip(sample_texts, untrained_preds_gru):
    emotions = dict(zip(emotion_cols, [round(float(p), 3) for p in pred]))
    print(f"\nText: {text}\nPredicted emotions: {emotions}")

print("\n=== Untrained GRU Evaluation ===")
loss_un_g, acc_un_g = base_model_gru.evaluate(X_val, y_val)
print(f"Untrained Loss: {loss_un_g:.4f}, Untrained Accuracy: {acc_un_g:.4f}")


# --- Train GRU Model ---
print("\n--- Training GRU ---")

model_gru = clone_model(base_model_gru)
model_gru.set_weights(base_model_gru.get_weights())

opt_clone_gru = Adam(learning_rate=0.0005)
model_gru.compile(loss='binary_crossentropy', optimizer=opt_clone_gru, metrics=['accuracy'])

early_stop_gru = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history_gru = model_gru.fit(
    X_train, y_train,
    epochs=25,
    batch_size=64,
    validation_data=(X_val, y_val),
    callbacks=[early_stop_gru],
    class_weight=class_weights
)


# --- Trained GRU Evaluation ---
print("\n=== Trained GRU Predictions (Sample Sentences) ===")
trained_preds_gru = model_gru.predict(sample_pad)
for text, pred in zip(sample_texts, trained_preds_gru):
    emotions = dict(zip(emotion_cols, [round(float(p), 3) for p in pred]))
    print(f"\nText: {text}\nPredicted emotions: {emotions}")

print("\n=== Trained GRU Evaluation ===")
loss_tr_g, acc_tr_g = model_gru.evaluate(X_val, y_val)
print(f"Trained Loss: {loss_tr_g:.4f}, Trained Accuracy: {acc_tr_g:.4f}")
print("---------------------------------")


# --- Visualization (LSTM vs GRU) ---
print("\n--- Training Charts (LSTM vs GRU) ---")

plt.figure(figsize=(14, 6))

# LSTM Loss
plt.subplot(1, 2, 1)
plt.plot(history_lstm.history['loss'], 'bo-', label='LSTM train')
plt.plot(history_lstm.history['val_loss'], 'ro-', label='LSTM val')
plt.plot(history_gru.history['loss'], 'b--', label='GRU train')
plt.plot(history_gru.history['val_loss'], 'r--', label='GRU val')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# LSTM Accuracy
plt.subplot(1, 2, 2)
plt.plot(history_lstm.history['accuracy'], 'bo-', label='LSTM train')
plt.plot(history_lstm.history['val_accuracy'], 'ro-', label='LSTM val')
plt.plot(history_gru.history['accuracy'], 'b--', label='GRU train')
plt.plot(history_gru.history['val_accuracy'], 'r--', label='GRU val')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.savefig('lstm_gru_training_charts.png')
print("Charts saved as 'lstm_gru_training_charts.png'")
plt.show()


